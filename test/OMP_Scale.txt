home machine: raid 0 for /:  cached reads at 6577.94MB/s, buffered reads at 133.88MB/s
              raid 1 for /mnt/data: cached reads at 6114.42MB/s, buffered reads at 65.35MB/s.

expected peformance if diskbound:  for 34MB file on /home/tpan/src/bliss, 0.25s
			           for 6GB file on /mnt/data/1000genome, 92s.


no master, 45e7b600f5eb2bd8e7db7b4be5283804793d5e7a,,, b17d9dda0f611231726111910705f3313317ba6b (HEAD),,:wq

testThreads numThreads, 1, 2, 3, 1, 2, 3
1, 2.02227,,,2.02321,,
2, 4.01112,,,2.13297,,
3, 5.93805,,,4.48951,,
4, 6.77488,,,6.48463,,
5, 7.23071,,,6.25194,,
6, 7.6358 ,,,7.10065,,
7, 7.66349,,,7.35539,,
8, 7.89757,,,7.53416,,

test1 (simple computation, OMP patterns) does not seem to scale at all.


lab machine: raid 0 for /:  cached reads at 12.679.05MB/s, buffered reads at 188.35MB/s
              raid 1 for /mnt/data: cached reads at 13200.90MB/s, buffered reads at 184.78MB/s.

expected peformance if diskbound:  for 34MB file on /home/tpan/src/bliss, 0.18s
			           for 6GB file on /mnt/data/1000genome, 32.47s.


no master, 45e7b600f5eb2bd8e7db7b4be5283804793d5e7a,,, b17d9dda0f611231726111910705f3313317ba6b (HEAD),,:wq

testThreads numThreads, 1, 2, 3, 1, 2, 3
1, 0.915671,,,0.925643,0.928003,
2, 1.42989 ,,,1.41417 ,1.39336 ,
3, 1.35917 ,,,1.47253 ,1.4588  ,
4, 1.28877 ,,,1.46118 ,1.4578  ,

testThreadsNoMPI numThreads, 1, 2, 3, 1, 2, 3
1, ,,, 0.406409,0.405374,
2, ,,, 0.679487,0.643044,
3, ,,, 0.813066,0.82492 ,
4, ,,, 0.837739,0.836073,

testThreadsNoQual numThreads, 1, 2, 3, 1, 2, 3
1, 0.219984,,, 0.233086,,
2, 0.577886,,, 0.4408  ,,
3, 0.942257,,, 0.903234,,
4, 0.99329 ,,, 0.967505,,

test1 (simple computation, OMP patterns) scales to about 3x at 4 cores.

With Chunking at 4K
testThreads numThreads, no master 1, 2, 3, master 1, 2, 3
1, 0.879457, , , 0.881537,,
2, 1.46507 , , , 1.42871 ,, 
3, 1.35424 , , , 1.34971 ,,
4, 1.29495 , , , 1.29015 ,,

with chunking at 8K.
testThreads numThreads, no master 1, 2, 3, MPI 1, 2, 3
1, 170.158, 170.299, , 170.158
2, 255.034, ,,         96.4753
3, 239.591,,,		   76.3815
4, 226.028,,,		   70.0483
8, ,,,				   59.6931

chunking at 800
TestThreadsNoMPI numThreads, no master 1, 2, 3, master 1, 2, 3, parfor 1, 2, 3
1, 0.389605,,, 0.389923,,, 0.39175 ,,
2, 0.798632,,, 0.741567,,, 0.763568,,
3, 0.863157,,, 0.856595,,, 0.864744,,
4, 0.893246,,, 0.887866,,, 0.888095,,

chunking at 800, parfor flushing.
TestThreadsNoQual numThreads, no master 1, 2, 3, master 1, 2, 3, parfor 1, 2, 3
1, 0.228352,,, 0.225831,,, 0.231862,,
2, 0.679307,,, 0.669486,,, 0.678584,,
3, 0.82312 ,,, 0.833834,,, 0.863313,,
4, 0.794383,,, 0.823936,,, 0.852337,,

chunking at 800, parfor flushing.
TestThreads numThreads, no master 1, 2, 3, master 1, 2, 3, parfor 1, 2, 3
1, 0.888397,,, 0.896556,,, 0.891037,,
2, 1.33968 ,,, 1.3375  ,,, 1.36265 ,,
3, 1.34608 ,,, 1.34597 ,,, 1.34179 ,,
4, 1.28536 ,,, 1.29006 ,,, 1.28848 ,,

chunking at 800, parfor flushing.
TestThreadsNoQualNoMPI numThreads, no master 1, 2, 3, master 1, 2, 3, parfor 1, 2, 3
1, 0.109098,,, 0.114513,,, 0.114172,,
2, 0.401848,,, 0.30203 ,,, 0.416725,,
3, 0.480617,,, 0.530177,,, 0.481208,,
4, 0.564438,,, 0.588648,,, 0.56211 ,,

chunking at 800, parfor flushing, no logging.  measured with "time"
TestThreadsNoQualNoMPI numThreads, no master 1, 2, 3, master 1, 2, 3, parfor 1, 2, 3
1, 0.113,,, 0.117,,, 0.112,,
2, 0.082,,, 0.088,,, 0.082,,
3, 0.074,,, 0.074,,, 0.070,,
4, 0.071,,, 0.082,,, 0.072 ,,



some thoughts:
1. critical section is very expensive:  a SO forum post claims 200x vs serial, where as atomic is 25x.  atomic would require atomic capture.
	another alternative is to use omp lock.  not sure the tradeoff
2. critical sections NEED TO BE NAMED  - no effect here since we don't use that many critical sections.
3. omp effectively has no thread sleep.
4. should be able to assign chunks using just the chunk size, without having do the search for boundaries in the critical section
5. may be good to limit the size of the chunk to page_size/num threads (theory, no multiple disk reads) - no effect. 
6. if using just chunk size, we are getting close to the structure of a parallel for loop, with dynamic or guided schedule.
7. tried it without quality score calculation:  0.23s, close to theoretical maximum with 1 thread. 
8. simple test (test1 == test.cpp) of omp generated 3x speed up.  critical and atomic appear to produce same timing. parfor is still fastest.
9. LOGGING was producing significant effect, at least for testThreadsNoQualNoMPI (scales a little bit to number of cores).  not observing this in other s. 